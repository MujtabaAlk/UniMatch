{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS 504 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "\n",
    "from dataset.semi import SemiDataset\n",
    "from model.semseg.deeplabv3plus import DeepLabV3Plus\n",
    "from no_distrib import evaluate\n",
    "from util.classes import CLASSES\n",
    "from util.ohem import ProbOhemCrossEntropy2d\n",
    "from util.utils import count_params, init_log, AverageMeter\n",
    "from util.dist_helper import setup_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='pascal'\n",
    "method='unimatch'\n",
    "exp='r101'\n",
    "split='732'\n",
    "\n",
    "config = f\"configs/{dataset}.yaml\"\n",
    "labeled_id_path = f\"splits/{dataset}/{split}/labeled.txt\"\n",
    "unlabeled_id_path = f\"splits/{dataset}/{split}/unlabeled.txt\"\n",
    "save_path = f\"exp/{dataset}/{method}/{exp}/{split}\"\n",
    "local_rank = 0\n",
    "port = 1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'pascal', 'nclass': 21, 'crop_size': 321, 'data_root': 'E:\\\\ICS_504\\\\project_datasets\\\\Pascal\\\\', 'epochs': 2, 'batch_size': 2, 'lr': 0.001, 'lr_multi': 10.0, 'criterion': {'name': 'CELoss', 'kwargs': {'ignore_index': 255}}, 'conf_thresh': 0.95, 'weak_threshold': 0.7, 'model': 'deeplabv3plus', 'backbone': 'resnet101', 'replace_stride_with_dilation': [False, False, True], 'dilations': [6, 12, 18]}\n"
     ]
    }
   ],
   "source": [
    "cfg = yaml.load(open(config, \"r\"), Loader=yaml.Loader)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = init_log('global', logging.INFO)\n",
    "logger.propagate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, world_size = setup_distributed(port=port)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "writer = SummaryWriter(save_path)\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 19:17:25,435][    INFO] Total params: 60.2M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DeepLabV3Plus(cfg)\n",
    "optimizer = SGD([{'params': model.backbone.parameters(), 'lr': cfg['lr']},\n",
    "                {'params': [param for name, param in model.named_parameters() if 'backbone' not in name],\n",
    "                'lr': cfg['lr'] * cfg['lr_multi']}], lr=cfg['lr'], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "model.cuda()\n",
    "\n",
    "# model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False,\n",
    "#                                                       output_device=local_rank, find_unused_parameters=False)\n",
    "logger.info('Total params: {:.1f}M\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['criterion']['name'] == 'CELoss':\n",
    "    criterion_l = nn.CrossEntropyLoss(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "elif cfg['criterion']['name'] == 'OHEM':\n",
    "    criterion_l = ProbOhemCrossEntropy2d(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "else:\n",
    "    raise NotImplementedError('%s criterion is not implemented' % cfg['criterion']['name'])\n",
    "\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none').cuda(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_u = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_u',\n",
    "                            cfg['crop_size'], unlabeled_id_path)\n",
    "trainset_l = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_l',\n",
    "                            cfg['crop_size'], labeled_id_path, nsample=len(trainset_u.ids))\n",
    "valset = SemiDataset(cfg['dataset'], cfg['data_root'], 'val')\n",
    "\n",
    "trainsampler_l = torch.utils.data.SequentialSampler(trainset_l)\n",
    "trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_l)\n",
    "trainsampler_u = torch.utils.data.SequentialSampler(trainset_u)\n",
    "trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_u)\n",
    "valsampler = torch.utils.data.SequentialSampler(valset)\n",
    "valloader = DataLoader(valset, batch_size=1, pin_memory=True, num_workers=1,\n",
    "                        drop_last=False, sampler=valsampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 19:17:25,460][    INFO] Total iters: 9850\n"
     ]
    }
   ],
   "source": [
    "total_iters = len(trainloader_u) * cfg['epochs']\n",
    "previous_best = 0.0\n",
    "epoch = -1\n",
    "\n",
    "logger.info(f\"Total iters: {total_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(save_path, 'latest.pth')):\n",
    "    checkpoint = torch.load(os.path.join(save_path, 'latest.pth'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    previous_best = checkpoint['previous_best']\n",
    "    \n",
    "    logger.info('************ Load from checkpoint at epoch %i\\n' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 19:17:25,486][    INFO] ===========> Epoch: 0, LR: 0.00100, Previous best: 0.00\n",
      "[2023-05-22 19:17:40,191][    INFO] Iters: 0, Total loss: 1.625, Loss x: 3.249, Loss s: 0.000, Loss w_fp: 0.000, Mask ratio: 0.000\n",
      "[2023-05-22 19:22:04,494][    INFO] Iters: 615, Total loss: 0.642, Loss x: 1.252, Loss s: 0.055, Loss w_fp: 0.009, Mask ratio: 0.232\n",
      "[2023-05-22 19:26:28,781][    INFO] Iters: 1230, Total loss: 0.556, Loss x: 1.071, Loss s: 0.075, Loss w_fp: 0.008, Mask ratio: 0.268\n",
      "[2023-05-22 19:30:52,870][    INFO] Iters: 1845, Total loss: 0.508, Loss x: 0.966, Loss s: 0.093, Loss w_fp: 0.008, Mask ratio: 0.293\n",
      "[2023-05-22 19:35:17,424][    INFO] Iters: 2460, Total loss: 0.472, Loss x: 0.888, Loss s: 0.105, Loss w_fp: 0.008, Mask ratio: 0.312\n",
      "[2023-05-22 19:39:41,268][    INFO] Iters: 3075, Total loss: 0.442, Loss x: 0.822, Loss s: 0.116, Loss w_fp: 0.009, Mask ratio: 0.334\n",
      "[2023-05-22 19:44:05,319][    INFO] Iters: 3690, Total loss: 0.420, Loss x: 0.773, Loss s: 0.125, Loss w_fp: 0.009, Mask ratio: 0.348\n",
      "[2023-05-22 19:48:29,416][    INFO] Iters: 4305, Total loss: 0.399, Loss x: 0.727, Loss s: 0.134, Loss w_fp: 0.010, Mask ratio: 0.364\n",
      "[2023-05-22 19:52:53,689][    INFO] Iters: 4920, Total loss: 0.381, Loss x: 0.687, Loss s: 0.142, Loss w_fp: 0.010, Mask ratio: 0.378\n",
      "[2023-05-22 19:56:05,538][    INFO] ***** Evaluation ***** >>>> Class [0 background] IoU: 86.76\n",
      "[2023-05-22 19:56:05,539][    INFO] ***** Evaluation ***** >>>> Class [1 aeroplane] IoU: 75.02\n",
      "[2023-05-22 19:56:05,539][    INFO] ***** Evaluation ***** >>>> Class [2 bicycle] IoU: 22.46\n",
      "[2023-05-22 19:56:05,540][    INFO] ***** Evaluation ***** >>>> Class [3 bird] IoU: 72.80\n",
      "[2023-05-22 19:56:05,540][    INFO] ***** Evaluation ***** >>>> Class [4 boat] IoU: 53.47\n",
      "[2023-05-22 19:56:05,541][    INFO] ***** Evaluation ***** >>>> Class [5 bottle] IoU: 52.86\n",
      "[2023-05-22 19:56:05,542][    INFO] ***** Evaluation ***** >>>> Class [6 bus] IoU: 76.40\n",
      "[2023-05-22 19:56:05,542][    INFO] ***** Evaluation ***** >>>> Class [7 car] IoU: 69.55\n",
      "[2023-05-22 19:56:05,543][    INFO] ***** Evaluation ***** >>>> Class [8 cat] IoU: 74.09\n",
      "[2023-05-22 19:56:05,543][    INFO] ***** Evaluation ***** >>>> Class [9 chair] IoU: 16.17\n",
      "[2023-05-22 19:56:05,544][    INFO] ***** Evaluation ***** >>>> Class [10 cow] IoU: 63.03\n",
      "[2023-05-22 19:56:05,544][    INFO] ***** Evaluation ***** >>>> Class [11 dining table] IoU: 33.55\n",
      "[2023-05-22 19:56:05,545][    INFO] ***** Evaluation ***** >>>> Class [12 dog] IoU: 64.71\n",
      "[2023-05-22 19:56:05,545][    INFO] ***** Evaluation ***** >>>> Class [13 horse] IoU: 59.27\n",
      "[2023-05-22 19:56:05,545][    INFO] ***** Evaluation ***** >>>> Class [14 motorbike] IoU: 58.79\n",
      "[2023-05-22 19:56:05,546][    INFO] ***** Evaluation ***** >>>> Class [15 person] IoU: 66.25\n",
      "[2023-05-22 19:56:05,546][    INFO] ***** Evaluation ***** >>>> Class [16 potted plant] IoU: 32.30\n",
      "[2023-05-22 19:56:05,546][    INFO] ***** Evaluation ***** >>>> Class [17 sheep] IoU: 65.73\n",
      "[2023-05-22 19:56:05,547][    INFO] ***** Evaluation ***** >>>> Class [18 sofa] IoU: 30.85\n",
      "[2023-05-22 19:56:05,548][    INFO] ***** Evaluation ***** >>>> Class [19 train] IoU: 68.75\n",
      "[2023-05-22 19:56:05,548][    INFO] ***** Evaluation ***** >>>> Class [20 tv/monitor] IoU: 40.89\n",
      "[2023-05-22 19:56:05,549][    INFO] ***** Evaluation original ***** >>>> MeanIoU: 56.37\n",
      "\n",
      "[2023-05-22 19:56:07,736][    INFO] ===========> Epoch: 1, LR: 0.00054, Previous best: 56.37\n",
      "[2023-05-22 19:56:11,691][    INFO] Iters: 0, Total loss: 0.350, Loss x: 0.572, Loss s: 0.252, Loss w_fp: 0.005, Mask ratio: 0.438\n",
      "[2023-05-22 20:00:31,944][    INFO] Iters: 615, Total loss: 0.230, Loss x: 0.358, Loss s: 0.189, Loss w_fp: 0.015, Mask ratio: 0.486\n",
      "[2023-05-22 20:04:52,261][    INFO] Iters: 1230, Total loss: 0.223, Loss x: 0.336, Loss s: 0.204, Loss w_fp: 0.015, Mask ratio: 0.494\n",
      "[2023-05-22 20:09:12,471][    INFO] Iters: 1845, Total loss: 0.212, Loss x: 0.314, Loss s: 0.206, Loss w_fp: 0.015, Mask ratio: 0.510\n",
      "[2023-05-22 20:13:32,827][    INFO] Iters: 2460, Total loss: 0.209, Loss x: 0.308, Loss s: 0.203, Loss w_fp: 0.016, Mask ratio: 0.520\n",
      "[2023-05-22 20:17:53,118][    INFO] Iters: 3075, Total loss: 0.202, Loss x: 0.294, Loss s: 0.204, Loss w_fp: 0.017, Mask ratio: 0.529\n",
      "[2023-05-22 20:22:13,322][    INFO] Iters: 3690, Total loss: 0.197, Loss x: 0.282, Loss s: 0.206, Loss w_fp: 0.017, Mask ratio: 0.536\n",
      "[2023-05-22 20:26:33,513][    INFO] Iters: 4305, Total loss: 0.191, Loss x: 0.272, Loss s: 0.205, Loss w_fp: 0.017, Mask ratio: 0.542\n",
      "[2023-05-22 20:30:53,805][    INFO] Iters: 4920, Total loss: 0.187, Loss x: 0.264, Loss s: 0.204, Loss w_fp: 0.017, Mask ratio: 0.547\n",
      "[2023-05-22 20:31:43,123][    INFO] ***** Evaluation ***** >>>> Class [0 background] IoU: 88.37\n",
      "[2023-05-22 20:31:43,124][    INFO] ***** Evaluation ***** >>>> Class [1 aeroplane] IoU: 72.69\n",
      "[2023-05-22 20:31:43,125][    INFO] ***** Evaluation ***** >>>> Class [2 bicycle] IoU: 39.17\n",
      "[2023-05-22 20:31:43,126][    INFO] ***** Evaluation ***** >>>> Class [3 bird] IoU: 80.45\n",
      "[2023-05-22 20:31:43,126][    INFO] ***** Evaluation ***** >>>> Class [4 boat] IoU: 55.58\n",
      "[2023-05-22 20:31:43,127][    INFO] ***** Evaluation ***** >>>> Class [5 bottle] IoU: 55.34\n",
      "[2023-05-22 20:31:43,128][    INFO] ***** Evaluation ***** >>>> Class [6 bus] IoU: 80.83\n",
      "[2023-05-22 20:31:43,128][    INFO] ***** Evaluation ***** >>>> Class [7 car] IoU: 74.79\n",
      "[2023-05-22 20:31:43,129][    INFO] ***** Evaluation ***** >>>> Class [8 cat] IoU: 79.09\n",
      "[2023-05-22 20:31:43,129][    INFO] ***** Evaluation ***** >>>> Class [9 chair] IoU: 20.57\n",
      "[2023-05-22 20:31:43,130][    INFO] ***** Evaluation ***** >>>> Class [10 cow] IoU: 74.68\n",
      "[2023-05-22 20:31:43,130][    INFO] ***** Evaluation ***** >>>> Class [11 dining table] IoU: 47.66\n",
      "[2023-05-22 20:31:43,131][    INFO] ***** Evaluation ***** >>>> Class [12 dog] IoU: 73.08\n",
      "[2023-05-22 20:31:43,131][    INFO] ***** Evaluation ***** >>>> Class [13 horse] IoU: 67.89\n",
      "[2023-05-22 20:31:43,132][    INFO] ***** Evaluation ***** >>>> Class [14 motorbike] IoU: 65.98\n",
      "[2023-05-22 20:31:43,133][    INFO] ***** Evaluation ***** >>>> Class [15 person] IoU: 76.35\n",
      "[2023-05-22 20:31:43,133][    INFO] ***** Evaluation ***** >>>> Class [16 potted plant] IoU: 42.50\n",
      "[2023-05-22 20:31:43,134][    INFO] ***** Evaluation ***** >>>> Class [17 sheep] IoU: 64.41\n",
      "[2023-05-22 20:31:43,135][    INFO] ***** Evaluation ***** >>>> Class [18 sofa] IoU: 31.84\n",
      "[2023-05-22 20:31:43,136][    INFO] ***** Evaluation ***** >>>> Class [19 train] IoU: 74.81\n",
      "[2023-05-22 20:31:43,136][    INFO] ***** Evaluation ***** >>>> Class [20 tv/monitor] IoU: 36.76\n",
      "[2023-05-22 20:31:43,137][    INFO] ***** Evaluation original ***** >>>> MeanIoU: 62.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch + 1, cfg['epochs']):\n",
    "    logger.info('===========> Epoch: {:}, LR: {:.5f}, Previous best: {:.2f}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], previous_best))\n",
    "\n",
    "    total_loss = AverageMeter()\n",
    "    total_loss_x = AverageMeter()\n",
    "    total_loss_s = AverageMeter()\n",
    "    total_loss_w_fp = AverageMeter()\n",
    "    total_mask_ratio = AverageMeter()\n",
    "\n",
    "    # trainloader_l.sampler.set_epoch(epoch)\n",
    "    # trainloader_u.sampler.set_epoch(epoch)\n",
    "\n",
    "    loader = zip(trainloader_l, trainloader_u, trainloader_u)\n",
    "\n",
    "    for i, ((img_x, mask_x),\n",
    "            (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),\n",
    "            (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) in enumerate(loader):\n",
    "        \n",
    "        img_x, mask_x = img_x.cuda(), mask_x.cuda()\n",
    "        img_u_w = img_u_w.cuda()\n",
    "        img_u_s1, img_u_s2, ignore_mask = img_u_s1.cuda(), img_u_s2.cuda(), ignore_mask.cuda()\n",
    "        cutmix_box1, cutmix_box2 = cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "        img_u_w_mix = img_u_w_mix.cuda()\n",
    "        img_u_s1_mix, img_u_s2_mix = img_u_s1_mix.cuda(), img_u_s2_mix.cuda()\n",
    "        ignore_mask_mix = ignore_mask_mix.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            pred_u_w_mix = model(img_u_w_mix).detach()\n",
    "            conf_u_w_mix = pred_u_w_mix.softmax(dim=1).max(dim=1)[0]\n",
    "            mask_u_w_mix = pred_u_w_mix.argmax(dim=1)\n",
    "\n",
    "        img_u_s1[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1] = \\\n",
    "            img_u_s1_mix[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1]\n",
    "        img_u_s2[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1] = \\\n",
    "            img_u_s2_mix[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1]\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        num_lb, num_ulb = img_x.shape[0], img_u_w.shape[0]\n",
    "\n",
    "        preds, preds_fp = model(torch.cat((img_x, img_u_w)), need_fp=True)\n",
    "        pred_x, pred_u_w = preds.split([num_lb, num_ulb])\n",
    "        pred_u_w_fp = preds_fp[num_lb:]\n",
    "\n",
    "        pred_u_s1, pred_u_s2 = model(torch.cat((img_u_s1, img_u_s2))).chunk(2)\n",
    "\n",
    "        pred_u_w = pred_u_w.detach()\n",
    "        conf_u_w = pred_u_w.softmax(dim=1).max(dim=1)[0]\n",
    "        mask_u_w = pred_u_w.argmax(dim=1)\n",
    "\n",
    "        mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "        mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "        mask_u_w_cutmixed1[cutmix_box1 == 1] = mask_u_w_mix[cutmix_box1 == 1]\n",
    "        conf_u_w_cutmixed1[cutmix_box1 == 1] = conf_u_w_mix[cutmix_box1 == 1]\n",
    "        ignore_mask_cutmixed1[cutmix_box1 == 1] = ignore_mask_mix[cutmix_box1 == 1]\n",
    "\n",
    "        mask_u_w_cutmixed2[cutmix_box2 == 1] = mask_u_w_mix[cutmix_box2 == 1]\n",
    "        conf_u_w_cutmixed2[cutmix_box2 == 1] = conf_u_w_mix[cutmix_box2 == 1]\n",
    "        ignore_mask_cutmixed2[cutmix_box2 == 1] = ignore_mask_mix[cutmix_box2 == 1]\n",
    "\n",
    "        loss_x = criterion_l(pred_x, mask_x)\n",
    "\n",
    "        loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "        loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= cfg['conf_thresh']) & (ignore_mask_cutmixed1 != 255))\n",
    "        loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "\n",
    "        loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "        loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= cfg['conf_thresh']) & (ignore_mask_cutmixed2 != 255))\n",
    "        loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "\n",
    "        loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)\n",
    "        loss_u_w_fp = loss_u_w_fp * ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255))\n",
    "        loss_u_w_fp = loss_u_w_fp.sum() / (ignore_mask != 255).sum().item()\n",
    "\n",
    "        loss = (loss_x + loss_u_s1 * 0.25 + loss_u_s2 * 0.25 + loss_u_w_fp * 0.5) / 2.0\n",
    "\n",
    "        # torch.distributed.barrier()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.update(loss.item())\n",
    "        total_loss_x.update(loss_x.item())\n",
    "        total_loss_s.update((loss_u_s1.item() + loss_u_s2.item()) / 2.0)\n",
    "        total_loss_w_fp.update(loss_u_w_fp.item())\n",
    "        \n",
    "        mask_ratio = ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255)).sum().item() / \\\n",
    "            (ignore_mask != 255).sum()\n",
    "        total_mask_ratio.update(mask_ratio.item())\n",
    "\n",
    "        iters = epoch * len(trainloader_u) + i\n",
    "        lr = cfg['lr'] * (1 - iters / total_iters) ** 0.9\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        optimizer.param_groups[1][\"lr\"] = lr * cfg['lr_multi']\n",
    "        \n",
    "        writer.add_scalar('train/loss_all', loss.item(), iters)\n",
    "        writer.add_scalar('train/loss_x', loss_x.item(), iters)\n",
    "        writer.add_scalar('train/loss_s', (loss_u_s1.item() + loss_u_s2.item()) / 2.0, iters)\n",
    "        writer.add_scalar('train/loss_w_fp', loss_u_w_fp.item(), iters)\n",
    "        writer.add_scalar('train/mask_ratio', mask_ratio, iters)\n",
    "    \n",
    "        if (i % (len(trainloader_u) // 8) == 0):\n",
    "            logger.info('Iters: {:}, Total loss: {:.3f}, Loss x: {:.3f}, Loss s: {:.3f}, Loss w_fp: {:.3f}, Mask ratio: '\n",
    "                        '{:.3f}'.format(i, total_loss.avg, total_loss_x.avg, total_loss_s.avg,\n",
    "                                        total_loss_w_fp.avg, total_mask_ratio.avg))\n",
    "\n",
    "    eval_mode = 'sliding_window' if cfg['dataset'] == 'cityscapes' else 'original'\n",
    "    mIoU, iou_class = evaluate(model, valloader, eval_mode, cfg)\n",
    "\n",
    "    for (cls_idx, iou) in enumerate(iou_class):\n",
    "        logger.info('***** Evaluation ***** >>>> Class [{:} {:}] '\n",
    "                    'IoU: {:.2f}'.format(cls_idx, CLASSES[cfg['dataset']][cls_idx], iou))\n",
    "    logger.info('***** Evaluation {} ***** >>>> MeanIoU: {:.2f}\\n'.format(eval_mode, mIoU))\n",
    "    \n",
    "    writer.add_scalar('eval/mIoU', mIoU, epoch)\n",
    "    for i, iou in enumerate(iou_class):\n",
    "        writer.add_scalar('eval/%s_IoU' % (CLASSES[cfg['dataset']][i]), iou, epoch)\n",
    "\n",
    "    is_best = mIoU > previous_best\n",
    "    previous_best = max(mIoU, previous_best)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'previous_best': previous_best,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_path, 'latest.pth'))\n",
    "    if is_best:\n",
    "        torch.save(checkpoint, os.path.join(save_path, 'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
