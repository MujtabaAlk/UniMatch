{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS 504 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "\n",
    "from dataset.semi import SemiDataset\n",
    "from model.semseg.deeplabv3plus import DeepLabV3Plus\n",
    "from no_distrib import evaluate\n",
    "from util.classes import CLASSES\n",
    "from util.ohem import ProbOhemCrossEntropy2d\n",
    "from util.utils import count_params, init_log, AverageMeter\n",
    "from util.dist_helper import setup_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='pascal'\n",
    "method='unimatch'\n",
    "exp='r101'\n",
    "split='732'\n",
    "\n",
    "config = f\"configs/{dataset}.yaml\"\n",
    "labeled_id_path = f\"splits/{dataset}/{split}/labeled.txt\"\n",
    "unlabeled_id_path = f\"splits/{dataset}/{split}/unlabeled.txt\"\n",
    "save_path = f\"exp/{dataset}/{method}/{exp}/{split}\"\n",
    "local_rank = 0\n",
    "port = 1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'pascal', 'nclass': 21, 'crop_size': 321, 'data_root': 'E:\\\\ICS_504\\\\project_datasets\\\\Pascal\\\\', 'epochs': 2, 'batch_size': 2, 'lr': 0.001, 'lr_multi': 10.0, 'criterion': {'name': 'CELoss', 'kwargs': {'ignore_index': 255}}, 'conf_thresh': 0.95, 'model': 'deeplabv3plus', 'backbone': 'resnet101', 'replace_stride_with_dilation': [False, False, True], 'dilations': [6, 12, 18]}\n"
     ]
    }
   ],
   "source": [
    "cfg = yaml.load(open(config, \"r\"), Loader=yaml.Loader)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = init_log('global', logging.INFO)\n",
    "logger.propagate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, world_size = setup_distributed(port=port)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "writer = SummaryWriter(save_path)\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-21 18:34:15,538][    INFO] Total params: 59.5M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DeepLabV3Plus(cfg)\n",
    "optimizer = SGD([{'params': model.backbone.parameters(), 'lr': cfg['lr']},\n",
    "                {'params': [param for name, param in model.named_parameters() if 'backbone' not in name],\n",
    "                'lr': cfg['lr'] * cfg['lr_multi']}], lr=cfg['lr'], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "model.cuda()\n",
    "\n",
    "# model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False,\n",
    "#                                                       output_device=local_rank, find_unused_parameters=False)\n",
    "logger.info('Total params: {:.1f}M\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['criterion']['name'] == 'CELoss':\n",
    "    criterion_l = nn.CrossEntropyLoss(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "elif cfg['criterion']['name'] == 'OHEM':\n",
    "    criterion_l = ProbOhemCrossEntropy2d(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "else:\n",
    "    raise NotImplementedError('%s criterion is not implemented' % cfg['criterion']['name'])\n",
    "\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none').cuda(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_u = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_u',\n",
    "                            cfg['crop_size'], unlabeled_id_path)\n",
    "trainset_l = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_l',\n",
    "                            cfg['crop_size'], labeled_id_path, nsample=len(trainset_u.ids))\n",
    "valset = SemiDataset(cfg['dataset'], cfg['data_root'], 'val')\n",
    "\n",
    "trainsampler_l = torch.utils.data.SequentialSampler(trainset_l)\n",
    "trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_l)\n",
    "trainsampler_u = torch.utils.data.SequentialSampler(trainset_u)\n",
    "trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_u)\n",
    "valsampler = torch.utils.data.SequentialSampler(valset)\n",
    "valloader = DataLoader(valset, batch_size=1, pin_memory=True, num_workers=1,\n",
    "                        drop_last=False, sampler=valsampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-21 18:34:15,568][    INFO] Total iters: 9850\n"
     ]
    }
   ],
   "source": [
    "total_iters = len(trainloader_u) * cfg['epochs']\n",
    "previous_best = 0.0\n",
    "epoch = -1\n",
    "\n",
    "logger.info(f\"Total iters: {total_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(save_path, 'latest.pth')):\n",
    "    checkpoint = torch.load(os.path.join(save_path, 'latest.pth'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    previous_best = checkpoint['previous_best']\n",
    "    \n",
    "    logger.info('************ Load from checkpoint at epoch %i\\n' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-21 18:34:15,596][    INFO] ===========> Epoch: 0, LR: 0.00100, Previous best: 0.00\n",
      "[2023-05-21 18:34:33,722][    INFO] Iters: 0, Total loss: 1.422, Loss x: 2.843, Loss s: 0.000, Loss w_fp: 0.000, Mask ratio: 0.000\n",
      "[2023-05-21 18:40:25,944][    INFO] Iters: 615, Total loss: 0.637, Loss x: 1.237, Loss s: 0.065, Loss w_fp: 0.009, Mask ratio: 0.246\n",
      "[2023-05-21 18:46:28,559][    INFO] Iters: 1230, Total loss: 0.569, Loss x: 1.092, Loss s: 0.084, Loss w_fp: 0.008, Mask ratio: 0.269\n",
      "[2023-05-21 18:52:30,206][    INFO] Iters: 1845, Total loss: 0.517, Loss x: 0.984, Loss s: 0.093, Loss w_fp: 0.008, Mask ratio: 0.289\n",
      "[2023-05-21 18:58:33,647][    INFO] Iters: 2460, Total loss: 0.487, Loss x: 0.920, Loss s: 0.100, Loss w_fp: 0.008, Mask ratio: 0.305\n",
      "[2023-05-21 19:04:40,866][    INFO] Iters: 3075, Total loss: 0.457, Loss x: 0.856, Loss s: 0.109, Loss w_fp: 0.008, Mask ratio: 0.324\n",
      "[2023-05-21 19:10:48,802][    INFO] Iters: 3690, Total loss: 0.431, Loss x: 0.798, Loss s: 0.118, Loss w_fp: 0.009, Mask ratio: 0.340\n",
      "[2023-05-21 19:16:51,025][    INFO] Iters: 4305, Total loss: 0.408, Loss x: 0.748, Loss s: 0.126, Loss w_fp: 0.010, Mask ratio: 0.356\n",
      "[2023-05-21 19:21:28,653][    INFO] Iters: 4920, Total loss: 0.387, Loss x: 0.702, Loss s: 0.134, Loss w_fp: 0.010, Mask ratio: 0.373\n",
      "[2023-05-21 19:24:44,373][    INFO] ***** Evaluation ***** >>>> Class [0 background] IoU: 87.13\n",
      "[2023-05-21 19:24:44,374][    INFO] ***** Evaluation ***** >>>> Class [1 aeroplane] IoU: 73.88\n",
      "[2023-05-21 19:24:44,375][    INFO] ***** Evaluation ***** >>>> Class [2 bicycle] IoU: 23.10\n",
      "[2023-05-21 19:24:44,375][    INFO] ***** Evaluation ***** >>>> Class [3 bird] IoU: 68.78\n",
      "[2023-05-21 19:24:44,376][    INFO] ***** Evaluation ***** >>>> Class [4 boat] IoU: 52.85\n",
      "[2023-05-21 19:24:44,376][    INFO] ***** Evaluation ***** >>>> Class [5 bottle] IoU: 52.04\n",
      "[2023-05-21 19:24:44,377][    INFO] ***** Evaluation ***** >>>> Class [6 bus] IoU: 75.06\n",
      "[2023-05-21 19:24:44,377][    INFO] ***** Evaluation ***** >>>> Class [7 car] IoU: 72.33\n",
      "[2023-05-21 19:24:44,378][    INFO] ***** Evaluation ***** >>>> Class [8 cat] IoU: 73.31\n",
      "[2023-05-21 19:24:44,378][    INFO] ***** Evaluation ***** >>>> Class [9 chair] IoU: 18.91\n",
      "[2023-05-21 19:24:44,379][    INFO] ***** Evaluation ***** >>>> Class [10 cow] IoU: 57.23\n",
      "[2023-05-21 19:24:44,379][    INFO] ***** Evaluation ***** >>>> Class [11 dining table] IoU: 45.65\n",
      "[2023-05-21 19:24:44,380][    INFO] ***** Evaluation ***** >>>> Class [12 dog] IoU: 60.55\n",
      "[2023-05-21 19:24:44,380][    INFO] ***** Evaluation ***** >>>> Class [13 horse] IoU: 44.99\n",
      "[2023-05-21 19:24:44,381][    INFO] ***** Evaluation ***** >>>> Class [14 motorbike] IoU: 55.74\n",
      "[2023-05-21 19:24:44,381][    INFO] ***** Evaluation ***** >>>> Class [15 person] IoU: 64.93\n",
      "[2023-05-21 19:24:44,382][    INFO] ***** Evaluation ***** >>>> Class [16 potted plant] IoU: 34.92\n",
      "[2023-05-21 19:24:44,382][    INFO] ***** Evaluation ***** >>>> Class [17 sheep] IoU: 50.90\n",
      "[2023-05-21 19:24:44,383][    INFO] ***** Evaluation ***** >>>> Class [18 sofa] IoU: 30.37\n",
      "[2023-05-21 19:24:44,384][    INFO] ***** Evaluation ***** >>>> Class [19 train] IoU: 63.27\n",
      "[2023-05-21 19:24:44,385][    INFO] ***** Evaluation ***** >>>> Class [20 tv/monitor] IoU: 28.81\n",
      "[2023-05-21 19:24:44,385][    INFO] ***** Evaluation original ***** >>>> MeanIoU: 54.04\n",
      "\n",
      "[2023-05-21 19:24:46,049][    INFO] ===========> Epoch: 1, LR: 0.00054, Previous best: 54.04\n",
      "[2023-05-21 19:24:50,912][    INFO] Iters: 0, Total loss: 0.396, Loss x: 0.425, Loss s: 0.703, Loss w_fp: 0.029, Mask ratio: 0.612\n",
      "[2023-05-21 19:29:13,615][    INFO] Iters: 615, Total loss: 0.227, Loss x: 0.346, Loss s: 0.197, Loss w_fp: 0.016, Mask ratio: 0.514\n",
      "[2023-05-21 19:33:36,309][    INFO] Iters: 1230, Total loss: 0.216, Loss x: 0.324, Loss s: 0.200, Loss w_fp: 0.016, Mask ratio: 0.515\n",
      "[2023-05-21 19:37:59,518][    INFO] Iters: 1845, Total loss: 0.210, Loss x: 0.312, Loss s: 0.198, Loss w_fp: 0.017, Mask ratio: 0.519\n",
      "[2023-05-21 19:42:22,353][    INFO] Iters: 2460, Total loss: 0.202, Loss x: 0.297, Loss s: 0.198, Loss w_fp: 0.017, Mask ratio: 0.530\n",
      "[2023-05-21 19:46:44,958][    INFO] Iters: 3075, Total loss: 0.195, Loss x: 0.283, Loss s: 0.197, Loss w_fp: 0.017, Mask ratio: 0.540\n",
      "[2023-05-21 19:51:07,504][    INFO] Iters: 3690, Total loss: 0.189, Loss x: 0.272, Loss s: 0.194, Loss w_fp: 0.017, Mask ratio: 0.546\n",
      "[2023-05-21 19:55:30,395][    INFO] Iters: 4305, Total loss: 0.185, Loss x: 0.264, Loss s: 0.194, Loss w_fp: 0.017, Mask ratio: 0.551\n",
      "[2023-05-21 19:59:53,306][    INFO] Iters: 4920, Total loss: 0.181, Loss x: 0.256, Loss s: 0.193, Loss w_fp: 0.017, Mask ratio: 0.555\n",
      "[2023-05-21 20:00:44,542][    INFO] ***** Evaluation ***** >>>> Class [0 background] IoU: 89.36\n",
      "[2023-05-21 20:00:44,543][    INFO] ***** Evaluation ***** >>>> Class [1 aeroplane] IoU: 70.16\n",
      "[2023-05-21 20:00:44,544][    INFO] ***** Evaluation ***** >>>> Class [2 bicycle] IoU: 42.09\n",
      "[2023-05-21 20:00:44,544][    INFO] ***** Evaluation ***** >>>> Class [3 bird] IoU: 76.84\n",
      "[2023-05-21 20:00:44,545][    INFO] ***** Evaluation ***** >>>> Class [4 boat] IoU: 56.08\n",
      "[2023-05-21 20:00:44,545][    INFO] ***** Evaluation ***** >>>> Class [5 bottle] IoU: 55.35\n",
      "[2023-05-21 20:00:44,546][    INFO] ***** Evaluation ***** >>>> Class [6 bus] IoU: 82.57\n",
      "[2023-05-21 20:00:44,546][    INFO] ***** Evaluation ***** >>>> Class [7 car] IoU: 75.94\n",
      "[2023-05-21 20:00:44,547][    INFO] ***** Evaluation ***** >>>> Class [8 cat] IoU: 76.72\n",
      "[2023-05-21 20:00:44,547][    INFO] ***** Evaluation ***** >>>> Class [9 chair] IoU: 24.44\n",
      "[2023-05-21 20:00:44,548][    INFO] ***** Evaluation ***** >>>> Class [10 cow] IoU: 67.59\n",
      "[2023-05-21 20:00:44,548][    INFO] ***** Evaluation ***** >>>> Class [11 dining table] IoU: 52.20\n",
      "[2023-05-21 20:00:44,549][    INFO] ***** Evaluation ***** >>>> Class [12 dog] IoU: 69.48\n",
      "[2023-05-21 20:00:44,550][    INFO] ***** Evaluation ***** >>>> Class [13 horse] IoU: 55.25\n",
      "[2023-05-21 20:00:44,550][    INFO] ***** Evaluation ***** >>>> Class [14 motorbike] IoU: 66.13\n",
      "[2023-05-21 20:00:44,551][    INFO] ***** Evaluation ***** >>>> Class [15 person] IoU: 76.37\n",
      "[2023-05-21 20:00:44,552][    INFO] ***** Evaluation ***** >>>> Class [16 potted plant] IoU: 34.94\n",
      "[2023-05-21 20:00:44,553][    INFO] ***** Evaluation ***** >>>> Class [17 sheep] IoU: 64.32\n",
      "[2023-05-21 20:00:44,553][    INFO] ***** Evaluation ***** >>>> Class [18 sofa] IoU: 32.65\n",
      "[2023-05-21 20:00:44,554][    INFO] ***** Evaluation ***** >>>> Class [19 train] IoU: 76.00\n",
      "[2023-05-21 20:00:44,554][    INFO] ***** Evaluation ***** >>>> Class [20 tv/monitor] IoU: 41.40\n",
      "[2023-05-21 20:00:44,555][    INFO] ***** Evaluation original ***** >>>> MeanIoU: 61.23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch + 1, cfg['epochs']):\n",
    "    logger.info('===========> Epoch: {:}, LR: {:.5f}, Previous best: {:.2f}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], previous_best))\n",
    "\n",
    "    total_loss = AverageMeter()\n",
    "    total_loss_x = AverageMeter()\n",
    "    total_loss_s = AverageMeter()\n",
    "    total_loss_w_fp = AverageMeter()\n",
    "    total_mask_ratio = AverageMeter()\n",
    "\n",
    "    # trainloader_l.sampler.set_epoch(epoch)\n",
    "    # trainloader_u.sampler.set_epoch(epoch)\n",
    "\n",
    "    loader = zip(trainloader_l, trainloader_u, trainloader_u)\n",
    "\n",
    "    for i, ((img_x, mask_x),\n",
    "            (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),\n",
    "            (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) in enumerate(loader):\n",
    "        \n",
    "        img_x, mask_x = img_x.cuda(), mask_x.cuda()\n",
    "        img_u_w = img_u_w.cuda()\n",
    "        img_u_s1, img_u_s2, ignore_mask = img_u_s1.cuda(), img_u_s2.cuda(), ignore_mask.cuda()\n",
    "        cutmix_box1, cutmix_box2 = cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "        img_u_w_mix = img_u_w_mix.cuda()\n",
    "        img_u_s1_mix, img_u_s2_mix = img_u_s1_mix.cuda(), img_u_s2_mix.cuda()\n",
    "        ignore_mask_mix = ignore_mask_mix.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            pred_u_w_mix = model(img_u_w_mix).detach()\n",
    "            conf_u_w_mix = pred_u_w_mix.softmax(dim=1).max(dim=1)[0]\n",
    "            mask_u_w_mix = pred_u_w_mix.argmax(dim=1)\n",
    "\n",
    "        img_u_s1[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1] = \\\n",
    "            img_u_s1_mix[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1]\n",
    "        img_u_s2[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1] = \\\n",
    "            img_u_s2_mix[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1]\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        num_lb, num_ulb = img_x.shape[0], img_u_w.shape[0]\n",
    "\n",
    "        preds, preds_fp = model(torch.cat((img_x, img_u_w)), True)\n",
    "        pred_x, pred_u_w = preds.split([num_lb, num_ulb])\n",
    "        pred_u_w_fp = preds_fp[num_lb:]\n",
    "\n",
    "        pred_u_s1, pred_u_s2 = model(torch.cat((img_u_s1, img_u_s2))).chunk(2)\n",
    "\n",
    "        pred_u_w = pred_u_w.detach()\n",
    "        conf_u_w = pred_u_w.softmax(dim=1).max(dim=1)[0]\n",
    "        mask_u_w = pred_u_w.argmax(dim=1)\n",
    "\n",
    "        mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "        mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "        mask_u_w_cutmixed1[cutmix_box1 == 1] = mask_u_w_mix[cutmix_box1 == 1]\n",
    "        conf_u_w_cutmixed1[cutmix_box1 == 1] = conf_u_w_mix[cutmix_box1 == 1]\n",
    "        ignore_mask_cutmixed1[cutmix_box1 == 1] = ignore_mask_mix[cutmix_box1 == 1]\n",
    "\n",
    "        mask_u_w_cutmixed2[cutmix_box2 == 1] = mask_u_w_mix[cutmix_box2 == 1]\n",
    "        conf_u_w_cutmixed2[cutmix_box2 == 1] = conf_u_w_mix[cutmix_box2 == 1]\n",
    "        ignore_mask_cutmixed2[cutmix_box2 == 1] = ignore_mask_mix[cutmix_box2 == 1]\n",
    "\n",
    "        loss_x = criterion_l(pred_x, mask_x)\n",
    "\n",
    "        loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "        loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= cfg['conf_thresh']) & (ignore_mask_cutmixed1 != 255))\n",
    "        loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "\n",
    "        loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "        loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= cfg['conf_thresh']) & (ignore_mask_cutmixed2 != 255))\n",
    "        loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "\n",
    "        loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)\n",
    "        loss_u_w_fp = loss_u_w_fp * ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255))\n",
    "        loss_u_w_fp = loss_u_w_fp.sum() / (ignore_mask != 255).sum().item()\n",
    "\n",
    "        loss = (loss_x + loss_u_s1 * 0.25 + loss_u_s2 * 0.25 + loss_u_w_fp * 0.5) / 2.0\n",
    "\n",
    "        # torch.distributed.barrier()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.update(loss.item())\n",
    "        total_loss_x.update(loss_x.item())\n",
    "        total_loss_s.update((loss_u_s1.item() + loss_u_s2.item()) / 2.0)\n",
    "        total_loss_w_fp.update(loss_u_w_fp.item())\n",
    "        \n",
    "        mask_ratio = ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255)).sum().item() / \\\n",
    "            (ignore_mask != 255).sum()\n",
    "        total_mask_ratio.update(mask_ratio.item())\n",
    "\n",
    "        iters = epoch * len(trainloader_u) + i\n",
    "        lr = cfg['lr'] * (1 - iters / total_iters) ** 0.9\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        optimizer.param_groups[1][\"lr\"] = lr * cfg['lr_multi']\n",
    "        \n",
    "        writer.add_scalar('train/loss_all', loss.item(), iters)\n",
    "        writer.add_scalar('train/loss_x', loss_x.item(), iters)\n",
    "        writer.add_scalar('train/loss_s', (loss_u_s1.item() + loss_u_s2.item()) / 2.0, iters)\n",
    "        writer.add_scalar('train/loss_w_fp', loss_u_w_fp.item(), iters)\n",
    "        writer.add_scalar('train/mask_ratio', mask_ratio, iters)\n",
    "    \n",
    "        if (i % (len(trainloader_u) // 8) == 0):\n",
    "            logger.info('Iters: {:}, Total loss: {:.3f}, Loss x: {:.3f}, Loss s: {:.3f}, Loss w_fp: {:.3f}, Mask ratio: '\n",
    "                        '{:.3f}'.format(i, total_loss.avg, total_loss_x.avg, total_loss_s.avg,\n",
    "                                        total_loss_w_fp.avg, total_mask_ratio.avg))\n",
    "\n",
    "    eval_mode = 'sliding_window' if cfg['dataset'] == 'cityscapes' else 'original'\n",
    "    mIoU, iou_class = evaluate(model, valloader, eval_mode, cfg)\n",
    "\n",
    "    for (cls_idx, iou) in enumerate(iou_class):\n",
    "        logger.info('***** Evaluation ***** >>>> Class [{:} {:}] '\n",
    "                    'IoU: {:.2f}'.format(cls_idx, CLASSES[cfg['dataset']][cls_idx], iou))\n",
    "    logger.info('***** Evaluation {} ***** >>>> MeanIoU: {:.2f}\\n'.format(eval_mode, mIoU))\n",
    "    \n",
    "    writer.add_scalar('eval/mIoU', mIoU, epoch)\n",
    "    for i, iou in enumerate(iou_class):\n",
    "        writer.add_scalar('eval/%s_IoU' % (CLASSES[cfg['dataset']][i]), iou, epoch)\n",
    "\n",
    "    is_best = mIoU > previous_best\n",
    "    previous_best = max(mIoU, previous_best)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'previous_best': previous_best,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_path, 'latest.pth'))\n",
    "    if is_best:\n",
    "        torch.save(checkpoint, os.path.join(save_path, 'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
