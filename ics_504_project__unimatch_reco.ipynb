{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS 504 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "\n",
    "from dataset.semi import SemiDataset\n",
    "from model.semseg.deeplabv3plus import DeepLabV3Plus\n",
    "from no_distrib import evaluate\n",
    "from util.classes import CLASSES\n",
    "from util.ohem import ProbOhemCrossEntropy2d\n",
    "from util.utils import count_params, init_log, AverageMeter\n",
    "from util.dist_helper import setup_distributed\n",
    "\n",
    "from module_list import *\n",
    "from build_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='pascal'\n",
    "method='unimatch_reco'\n",
    "exp='r101'\n",
    "split='732'\n",
    "\n",
    "config = f\"configs/{dataset}.yaml\"\n",
    "labeled_id_path = f\"splits/{dataset}/{split}/labeled.txt\"\n",
    "unlabeled_id_path = f\"splits/{dataset}/{split}/unlabeled.txt\"\n",
    "save_path = f\"exp/{dataset}/{method}/{exp}/{split}\"\n",
    "local_rank = 0\n",
    "port = 1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'pascal', 'nclass': 21, 'crop_size': 321, 'data_root': 'E:\\\\ICS_504\\\\project_datasets\\\\Pascal\\\\', 'epochs': 2, 'batch_size': 2, 'lr': 0.001, 'lr_multi': 10.0, 'criterion': {'name': 'CELoss', 'kwargs': {'ignore_index': 255}}, 'conf_thresh': 0.95, 'weak_threshold': 0.7, 'model': 'deeplabv3plus', 'backbone': 'resnet101', 'replace_stride_with_dilation': [False, False, True], 'dilations': [6, 12, 18]}\n"
     ]
    }
   ],
   "source": [
    "cfg = yaml.load(open(config, \"r\"), Loader=yaml.Loader)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = init_log('global', logging.INFO)\n",
    "logger.propagate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, world_size = setup_distributed(port=port)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "writer = SummaryWriter(save_path)\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 22:45:44,528][    INFO] Total params: 60.2M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DeepLabV3Plus(cfg)\n",
    "optimizer = SGD([{'params': model.backbone.parameters(), 'lr': cfg['lr']},\n",
    "                {'params': [param for name, param in model.named_parameters() if 'backbone' not in name],\n",
    "                'lr': cfg['lr'] * cfg['lr_multi']}], lr=cfg['lr'], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "model.cuda()\n",
    "\n",
    "# model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False,\n",
    "#                                                       output_device=local_rank, find_unused_parameters=False)\n",
    "\n",
    "\n",
    "ema = EMA(model, 0.99)  # Mean teacher model\n",
    "\n",
    "logger.info('Total params: {:.1f}M\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['criterion']['name'] == 'CELoss':\n",
    "    criterion_l = nn.CrossEntropyLoss(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "elif cfg['criterion']['name'] == 'OHEM':\n",
    "    criterion_l = ProbOhemCrossEntropy2d(**cfg['criterion']['kwargs']).cuda(local_rank)\n",
    "else:\n",
    "    raise NotImplementedError('%s criterion is not implemented' % cfg['criterion']['name'])\n",
    "\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none').cuda(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_u = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_u',\n",
    "                            cfg['crop_size'], unlabeled_id_path)\n",
    "trainset_l = SemiDataset(cfg['dataset'], cfg['data_root'], 'train_l',\n",
    "                            cfg['crop_size'], labeled_id_path, nsample=len(trainset_u.ids))\n",
    "valset = SemiDataset(cfg['dataset'], cfg['data_root'], 'val')\n",
    "\n",
    "trainsampler_l = torch.utils.data.SequentialSampler(trainset_l)\n",
    "trainloader_l = DataLoader(trainset_l, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_l)\n",
    "trainsampler_u = torch.utils.data.SequentialSampler(trainset_u)\n",
    "trainloader_u = DataLoader(trainset_u, batch_size=cfg['batch_size'],\n",
    "                            pin_memory=True, num_workers=1, drop_last=True, sampler=trainsampler_u)\n",
    "valsampler = torch.utils.data.SequentialSampler(valset)\n",
    "valloader = DataLoader(valset, batch_size=1, pin_memory=True, num_workers=1,\n",
    "                        drop_last=False, sampler=valsampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 22:45:44,556][    INFO] Total iters: 9850\n"
     ]
    }
   ],
   "source": [
    "total_iters = len(trainloader_u) * cfg['epochs']\n",
    "previous_best = 0.0\n",
    "epoch = -1\n",
    "\n",
    "logger.info(f\"Total iters: {total_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(save_path, 'latest.pth')):\n",
    "    checkpoint = torch.load(os.path.join(save_path, 'latest.pth'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    previous_best = checkpoint['previous_best']\n",
    "    \n",
    "    logger.info('************ Load from checkpoint at epoch %i\\n' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-22 22:45:44,644][    INFO] ===========> Epoch: 0, LR: 0.00100, Previous best: 0.00\n",
      "E:\\ICS_504\\UniMatch\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [8, 321, 321] at index 0 does not match the shape of the indexed tensor [12, 81, 81, 256] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 121\u001b[0m\n\u001b[0;32m    117\u001b[0m     prob_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prob_l, prob_u))\n\u001b[0;32m    119\u001b[0m     rep_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((rep_xu, rep_s1s2))\n\u001b[1;32m--> 121\u001b[0m     reco_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_reco_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrep_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconf_thresh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m (loss_x \u001b[38;5;241m+\u001b[39m loss_u_s1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m loss_u_s2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m loss_u_w_fp \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# torch.distributed.barrier()\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ICS_504\\UniMatch\\module_list.py:89\u001b[0m, in \u001b[0;36mcompute_reco_loss\u001b[1;34m(rep, label, mask, prob, strong_threshold, temp, num_queries, num_negatives)\u001b[0m\n\u001b[0;32m     86\u001b[0m prob_seg \u001b[38;5;241m=\u001b[39m prob[:, i, :, :]\n\u001b[0;32m     87\u001b[0m rep_mask_hard \u001b[38;5;241m=\u001b[39m (prob_seg \u001b[38;5;241m<\u001b[39m strong_threshold) \u001b[38;5;241m*\u001b[39m valid_pixel_seg\u001b[38;5;241m.\u001b[39mbool()  \u001b[38;5;66;03m# select hard queries\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m seg_proto_list\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mrep\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_pixel_seg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     90\u001b[0m seg_feat_all_list\u001b[38;5;241m.\u001b[39mappend(rep[valid_pixel_seg\u001b[38;5;241m.\u001b[39mbool()])\n\u001b[0;32m     91\u001b[0m seg_feat_hard_list\u001b[38;5;241m.\u001b[39mappend(rep[rep_mask_hard])\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [8, 321, 321] at index 0 does not match the shape of the indexed tensor [12, 81, 81, 256] at index 0"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch + 1, cfg['epochs']):\n",
    "    logger.info('===========> Epoch: {:}, LR: {:.5f}, Previous best: {:.2f}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], previous_best))\n",
    "\n",
    "    total_loss = AverageMeter()\n",
    "    total_loss_x = AverageMeter()\n",
    "    total_loss_s = AverageMeter()\n",
    "    total_loss_w_fp = AverageMeter()\n",
    "    total_mask_ratio = AverageMeter()\n",
    "\n",
    "    # trainloader_l.sampler.set_epoch(epoch)\n",
    "    # trainloader_u.sampler.set_epoch(epoch)\n",
    "\n",
    "    loader = zip(trainloader_l, trainloader_u, trainloader_u)\n",
    "\n",
    "    for i, ((img_x, mask_x),\n",
    "            (img_u_w, img_u_s1, img_u_s2, ignore_mask, cutmix_box1, cutmix_box2),\n",
    "            (img_u_w_mix, img_u_s1_mix, img_u_s2_mix, ignore_mask_mix, _, _)) in enumerate(loader):\n",
    "        \n",
    "        img_x, mask_x = img_x.cuda(), mask_x.cuda()\n",
    "        img_u_w = img_u_w.cuda()\n",
    "        img_u_s1, img_u_s2, ignore_mask = img_u_s1.cuda(), img_u_s2.cuda(), ignore_mask.cuda()\n",
    "        cutmix_box1, cutmix_box2 = cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "        img_u_w_mix = img_u_w_mix.cuda()\n",
    "        img_u_s1_mix, img_u_s2_mix = img_u_s1_mix.cuda(), img_u_s2_mix.cuda()\n",
    "        ignore_mask_mix = ignore_mask_mix.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            pred_u_w_mix = model(img_u_w_mix).detach()\n",
    "            conf_u_w_mix = pred_u_w_mix.softmax(dim=1).max(dim=1)[0]\n",
    "            mask_u_w_mix = pred_u_w_mix.argmax(dim=1)\n",
    "\n",
    "        img_u_s1[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1] = \\\n",
    "            img_u_s1_mix[cutmix_box1.unsqueeze(1).expand(img_u_s1.shape) == 1]\n",
    "        img_u_s2[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1] = \\\n",
    "            img_u_s2_mix[cutmix_box2.unsqueeze(1).expand(img_u_s2.shape) == 1]\n",
    "\n",
    "        model.train()\n",
    "        ema.model.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_u_data = torch.cat((img_u_w, img_u_s1, img_u_s2))            \n",
    "            pred_u = ema.model(train_u_data)            \n",
    "            pred_u_large_raw = F.interpolate(pred_u, size=ignore_mask.shape[1:], mode=\"bilinear\", align_corners=True)\n",
    "            pseudo_logits, pseudo_labels = torch.max(torch.softmax(pred_u_large_raw, dim=1), dim=1)\n",
    "\n",
    "             # random scale images first\n",
    "            train_u_aug_data, train_u_aug_label, train_u_aug_logits = \\\n",
    "                batch_transform(train_u_data, pseudo_labels, pseudo_logits,\n",
    "                                [321, 321], (0.5, 1.5), apply_augmentation=False)\n",
    "\n",
    "            # apply mixing strategy: cutout, cutmix or classmix\n",
    "            train_u_aug_data, train_u_aug_label, train_u_aug_logits = \\\n",
    "                generate_unsup_data(train_u_aug_data, train_u_aug_label, train_u_aug_logits, mode=\"cutout\")\n",
    "\n",
    "            # apply augmentation: color jitter + flip + gaussian blur\n",
    "            train_u_aug_data, train_u_aug_label, train_u_aug_logits = \\\n",
    "                batch_transform(train_u_aug_data, train_u_aug_label, train_u_aug_logits,\n",
    "                                [321, 321], (1.0, 1.0), apply_augmentation=True)\n",
    "\n",
    "        num_lb, num_ulb = img_x.shape[0], img_u_w.shape[0]\n",
    "\n",
    "        preds, preds_fp, rep_xu = model(torch.cat((img_x, img_u_w)), need_fp=True, need_rep=True)\n",
    "        pred_x, pred_u_w = preds.split([num_lb, num_ulb])\n",
    "        pred_u_w_fp = preds_fp[num_lb:]\n",
    "\n",
    "        pred_u_s1s2, rep_s1s2 = model(torch.cat((img_u_s1, img_u_s2)), need_rep=True)\n",
    "        pred_u_s1, pred_u_s2 = pred_u_s1s2.chunk(2)\n",
    "\n",
    "        pred_u_w = pred_u_w.detach()\n",
    "        conf_u_w = pred_u_w.softmax(dim=1).max(dim=1)[0]\n",
    "        mask_u_w = pred_u_w.argmax(dim=1)\n",
    "\n",
    "        mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "        mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \\\n",
    "            mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "        mask_u_w_cutmixed1[cutmix_box1 == 1] = mask_u_w_mix[cutmix_box1 == 1]\n",
    "        conf_u_w_cutmixed1[cutmix_box1 == 1] = conf_u_w_mix[cutmix_box1 == 1]\n",
    "        ignore_mask_cutmixed1[cutmix_box1 == 1] = ignore_mask_mix[cutmix_box1 == 1]\n",
    "\n",
    "        mask_u_w_cutmixed2[cutmix_box2 == 1] = mask_u_w_mix[cutmix_box2 == 1]\n",
    "        conf_u_w_cutmixed2[cutmix_box2 == 1] = conf_u_w_mix[cutmix_box2 == 1]\n",
    "        ignore_mask_cutmixed2[cutmix_box2 == 1] = ignore_mask_mix[cutmix_box2 == 1]\n",
    "\n",
    "        loss_x = criterion_l(pred_x, mask_x)\n",
    "\n",
    "        loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "        loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= cfg['conf_thresh']) & (ignore_mask_cutmixed1 != 255))\n",
    "        loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "\n",
    "        loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "        loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= cfg['conf_thresh']) & (ignore_mask_cutmixed2 != 255))\n",
    "        loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "\n",
    "        loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)\n",
    "        loss_u_w_fp = loss_u_w_fp * ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255))\n",
    "        loss_u_w_fp = loss_u_w_fp.sum() / (ignore_mask != 255).sum().item()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_all = torch.cat((preds, pred_u_s1, pred_u_s2))\n",
    "            train_u_aug_mask = train_u_aug_logits.ge(cfg['weak_threshold']).float()\n",
    "            mask_all = torch.cat(((mask_x.unsqueeze(1) >= 0).float(), train_u_aug_mask.unsqueeze(1)))\n",
    "            mask_all = F.interpolate(mask_all, size=pred_all.shape[2:], mode='nearest')\n",
    "\n",
    "            label_l = F.interpolate(F.one_hot(mask_x).to(torch.float), size=pred_all.shape[2:], mode='nearest')\n",
    "            train_u_aug_label[train_u_aug_label < 0] = 0\n",
    "            label_u = F.interpolate(F.one_hot(train_u_aug_label).to(torch.float), size=pred_all.shape[2:], mode='nearest')\n",
    "            label_all = torch.cat((label_l, label_u))\n",
    "\n",
    "            prob_l = torch.softmax(pred_x, dim=1)\n",
    "            prob_u = torch.softmax(pred_u, dim=1)\n",
    "            prob_all = torch.cat((prob_l, prob_u))\n",
    "\n",
    "            rep_all = torch.cat((rep_xu, rep_s1s2))\n",
    "\n",
    "            reco_loss = compute_reco_loss(rep_all, label_all, mask_all, prob_all, cfg['conf_thresh'],\n",
    "                                          0.5, 256, 512)\n",
    "\n",
    "\n",
    "\n",
    "        loss = (loss_x + loss_u_s1 * 0.25 + loss_u_s2 * 0.25 + loss_u_w_fp * 0.5) / 2.0\n",
    "\n",
    "        # torch.distributed.barrier()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.update(loss.item())\n",
    "        total_loss_x.update(loss_x.item())\n",
    "        total_loss_s.update((loss_u_s1.item() + loss_u_s2.item()) / 2.0)\n",
    "        total_loss_w_fp.update(loss_u_w_fp.item())\n",
    "        \n",
    "        mask_ratio = ((conf_u_w >= cfg['conf_thresh']) & (ignore_mask != 255)).sum().item() / \\\n",
    "            (ignore_mask != 255).sum()\n",
    "        total_mask_ratio.update(mask_ratio.item())\n",
    "\n",
    "        iters = epoch * len(trainloader_u) + i\n",
    "        lr = cfg['lr'] * (1 - iters / total_iters) ** 0.9\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        optimizer.param_groups[1][\"lr\"] = lr * cfg['lr_multi']\n",
    "        \n",
    "        writer.add_scalar('train/loss_all', loss.item(), iters)\n",
    "        writer.add_scalar('train/loss_x', loss_x.item(), iters)\n",
    "        writer.add_scalar('train/loss_s', (loss_u_s1.item() + loss_u_s2.item()) / 2.0, iters)\n",
    "        writer.add_scalar('train/loss_w_fp', loss_u_w_fp.item(), iters)\n",
    "        writer.add_scalar('train/mask_ratio', mask_ratio, iters)\n",
    "    \n",
    "        if (i % (len(trainloader_u) // 8) == 0):\n",
    "            logger.info('Iters: {:}, Total loss: {:.3f}, Loss x: {:.3f}, Loss s: {:.3f}, Loss w_fp: {:.3f}, Mask ratio: '\n",
    "                        '{:.3f}'.format(i, total_loss.avg, total_loss_x.avg, total_loss_s.avg,\n",
    "                                        total_loss_w_fp.avg, total_mask_ratio.avg))\n",
    "\n",
    "    eval_mode = 'sliding_window' if cfg['dataset'] == 'cityscapes' else 'original'\n",
    "    mIoU, iou_class = evaluate(model, valloader, eval_mode, cfg)\n",
    "\n",
    "    for (cls_idx, iou) in enumerate(iou_class):\n",
    "        logger.info('***** Evaluation ***** >>>> Class [{:} {:}] '\n",
    "                    'IoU: {:.2f}'.format(cls_idx, CLASSES[cfg['dataset']][cls_idx], iou))\n",
    "    logger.info('***** Evaluation {} ***** >>>> MeanIoU: {:.2f}\\n'.format(eval_mode, mIoU))\n",
    "    \n",
    "    writer.add_scalar('eval/mIoU', mIoU, epoch)\n",
    "    for i, iou in enumerate(iou_class):\n",
    "        writer.add_scalar('eval/%s_IoU' % (CLASSES[cfg['dataset']][i]), iou, epoch)\n",
    "\n",
    "    is_best = mIoU > previous_best\n",
    "    previous_best = max(mIoU, previous_best)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'previous_best': previous_best,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_path, 'latest.pth'))\n",
    "    if is_best:\n",
    "        torch.save(checkpoint, os.path.join(save_path, 'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
